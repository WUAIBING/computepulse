#!/usr/bin/env python3
"""
Optimized price fetching script with improved error handling,
data validation, and merge logic.

Data Sources:
1. Qwen (é€šä¹‰åƒé—®) - Fast, reliable, with search
2. DeepSeek - Fast, with reasoning mode
3. Doubao (è±†åŒ…) - Optional, slow but has web_search tool
"""

import dashscope
import json
import os
import sys
import time
import requests
from datetime import datetime, timedelta
from dashscope import Generation
from openai import OpenAI
from typing import List, Dict, Optional, Any

# Set API Keys
dashscope.api_key = os.getenv('DASHSCOPE_API_KEY')
volc_api_key = os.getenv('VOLC_API_KEY') or os.getenv('VITE_VOLC_API_KEY')

# Try to load from .env.local if not in env
if not dashscope.api_key or not volc_api_key:
    try:
        with open('.env.local', 'r') as f:
            for line in f:
                if 'DASHSCOPE_API_KEY' in line:
                    dashscope.api_key = line.split('=')[1].strip()
                if 'VOLC_API_KEY' in line:
                    volc_api_key = line.split('=')[1].strip()
    except:
        pass

# Initialize DeepSeek client (via Alibaba Cloud DashScope)
deepseek_client = None
if dashscope.api_key:
    try:
        deepseek_client = OpenAI(
            api_key=dashscope.api_key,
            base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
        )
    except Exception as e:
        print(f"[{datetime.now()}] Warning: Failed to initialize DeepSeek client: {e}")

DOUBAO_ENDPOINT_ID = "doubao-seed-1-6-251015"  # Confirmed from API example
ENABLE_DOUBAO = False  # Disabled by default due to slow response time

# Data validation thresholds
GPU_PRICE_MIN = 0.1  # Minimum reasonable GPU price per hour (USD)
GPU_PRICE_MAX = 50.0  # Maximum reasonable GPU price per hour (USD)
TOKEN_PRICE_MIN = 0.001  # Minimum reasonable token price per 1M (USD)
TOKEN_PRICE_MAX = 100.0  # Maximum reasonable token price per 1M (USD)
KWH_PRICE_MIN = 0.01  # Minimum reasonable electricity price (USD/kWh)
KWH_PRICE_MAX = 1.0  # Maximum reasonable electricity price (USD/kWh)

def get_doubao_endpoint():
    """Get Doubao endpoint ID with caching."""
    global DOUBAO_ENDPOINT_ID
    if DOUBAO_ENDPOINT_ID:
        return DOUBAO_ENDPOINT_ID
        
    if not volc_api_key:
        return None
        
    try:
        url = "https://ark.cn-beijing.volces.com/api/v3/inference_endpoints?page_num=1&page_size=20"
        headers = {"Authorization": f"Bearer {volc_api_key}"}
        response = requests.get(url, headers=headers, timeout=10)
        
        if response.status_code == 200:
            data = response.json()
            if 'items' in data:
                for item in data['items']:
                    if item.get('status') == 'Running':
                        model_ref = item.get('model_reference', {}).get('model_id', '').lower()
                        if 'doubao' in model_ref:
                            DOUBAO_ENDPOINT_ID = item['id']
                            print(f"[{datetime.now()}] Found Doubao Endpoint: {DOUBAO_ENDPOINT_ID}")
                            return DOUBAO_ENDPOINT_ID
                            
                # Fallback to first running endpoint
                for item in data['items']:
                    if item.get('status') == 'Running':
                        DOUBAO_ENDPOINT_ID = item['id']
                        print(f"[{datetime.now()}] Using fallback Endpoint: {DOUBAO_ENDPOINT_ID}")
                        return DOUBAO_ENDPOINT_ID
                        
        print(f"[{datetime.now()}] Warning: No active Doubao endpoint found.")
    except Exception as e:
        print(f"[{datetime.now()}] Error fetching Doubao endpoint: {e}")
    return None

def call_doubao_with_search(prompt: str, max_retries: int = 2, reasoning_effort: str = "low") -> Optional[str]:
    """
    Call Doubao API with web search enabled using responses API.
    Includes retry logic for better reliability.
    
    Args:
        prompt: The prompt to send to the API
        max_retries: Maximum number of retry attempts
        reasoning_effort: Not used for responses API (only chat/completions supports it)
    """
    endpoint_id = DOUBAO_ENDPOINT_ID or get_doubao_endpoint()
    if not endpoint_id:
        return None
    
    headers = {
        "Authorization": f"Bearer {volc_api_key}",
        "Content-Type": "application/json"
    }
    
    for attempt in range(max_retries):
        try:
            # Use responses API with web_search tool for real-time data
            url = "https://ark.cn-beijing.volces.com/api/v3/responses"
            payload = {
                "model": endpoint_id,
                "stream": False,
                "tools": [{"type": "web_search"}],  # Enable web search
                "input": [
                    {
                        "role": "user",
                        "content": [{"type": "input_text", "text": prompt}]
                    }
                ]
            }
            
            response = requests.post(url, headers=headers, json=payload, timeout=120)
            if response.status_code == 200:
                res_json = response.json()
                # Extract message content from output
                if 'output' in res_json:
                    for item in res_json['output']:
                        if item.get('type') == 'message' and 'content' in item:
                            for content_item in item['content']:
                                # Handle both 'text' and 'output_text' types
                                if content_item.get('type') in ['text', 'output_text']:
                                    text = content_item.get('text') or content_item.get('output_text')
                                    if text:
                                        return text
            
            if response.status_code != 200:
                print(f"[{datetime.now()}] Doubao API Error (attempt {attempt+1}): {response.status_code}")
                if attempt < max_retries - 1:
                    time.sleep(2 ** attempt)  # Exponential backoff
                    
        except Exception as e:
            print(f"[{datetime.now()}] Doubao Call Failed (attempt {attempt+1}): {e}")
            if attempt < max_retries - 1:
                time.sleep(2 ** attempt)
    
    return None

def call_qwen_with_search(prompt: str, max_retries: int = 2) -> Optional[str]:
    """
    Call Qwen API with search enabled.
    Includes retry logic for better reliability.
    """
    for attempt in range(max_retries):
        try:
            response = Generation.call(
                model='qwen-max',
                prompt=prompt,
                enable_search=True
            )
            if response.status_code == 200:
                return response.output.text
            else:
                print(f"[{datetime.now()}] Qwen API Error (attempt {attempt+1}): {response.message}")
                if attempt < max_retries - 1:
                    time.sleep(2 ** attempt)
        except Exception as e:
            print(f"[{datetime.now()}] Qwen Call Failed (attempt {attempt+1}): {e}")
            if attempt < max_retries - 1:
                time.sleep(2 ** attempt)
    
    return None

def call_deepseek_with_reasoning(prompt: str, max_retries: int = 2) -> Optional[str]:
    """
    Call DeepSeek API with reasoning mode enabled.
    Uses OpenAI-compatible API via Alibaba Cloud DashScope.
    Includes retry logic for better reliability.
    """
    if not deepseek_client:
        return None
    
    for attempt in range(max_retries):
        try:
            completion = deepseek_client.chat.completions.create(
                model="deepseek-v3.2-exp",
                messages=[{"role": "user", "content": prompt}],
                extra_body={"enable_thinking": False},  # Disable thinking for faster response
                stream=False,
            )
            
            if completion.choices and len(completion.choices) > 0:
                return completion.choices[0].message.content
            else:
                print(f"[{datetime.now()}] DeepSeek API Error (attempt {attempt+1}): No response")
                if attempt < max_retries - 1:
                    time.sleep(2 ** attempt)
                    
        except Exception as e:
            print(f"[{datetime.now()}] DeepSeek Call Failed (attempt {attempt+1}): {e}")
            if attempt < max_retries - 1:
                time.sleep(2 ** attempt)
    
    return None

def clean_and_parse_json(content: Optional[str]) -> Optional[Any]:
    """Parse JSON from LLM response with better error handling."""
    if not content:
        return None
    
    try:
        # Remove markdown code blocks
        cleaned = content.replace('```json', '').replace('```', '').strip()
        
        # Extract JSON structure
        if '[' in cleaned and ']' in cleaned:
            start = cleaned.find('[')
            end = cleaned.rfind(']') + 1
            cleaned = cleaned[start:end]
        elif '{' in cleaned and '}' in cleaned:
            start = cleaned.find('{')
            end = cleaned.rfind('}') + 1
            cleaned = cleaned[start:end]
        else:
            print(f"[{datetime.now()}] No JSON structure found in response")
            return None
            
        return json.loads(cleaned)
    except json.JSONDecodeError as e:
        print(f"[{datetime.now()}] JSON Parse Error: {e}")
        print(f"Content snippet: {content[:200] if content else 'None'}")
        return None
    except Exception as e:
        print(f"[{datetime.now()}] Unexpected error parsing JSON: {e}")
        return None

def validate_gpu_price(item: Dict) -> bool:
    """Validate GPU price data."""
    required_fields = ['provider', 'gpu', 'price']
    
    # Check required fields
    for field in required_fields:
        if field not in item:
            print(f"[{datetime.now()}] Missing required field '{field}' in GPU data")
            return False
    
    # Validate price range
    price = item.get('price', 0)
    if not isinstance(price, (int, float)):
        print(f"[{datetime.now()}] Invalid price type for {item.get('provider')}: {type(price)}")
        return False
    
    if not (GPU_PRICE_MIN <= price <= GPU_PRICE_MAX):
        print(f"[{datetime.now()}] Price out of range for {item.get('provider')}: ${price}")
        return False
    
    return True

def validate_token_price(item: Dict) -> bool:
    """Validate token price data."""
    required_fields = ['provider', 'model', 'input_price', 'output_price']
    
    # Check required fields
    for field in required_fields:
        if field not in item:
            print(f"[{datetime.now()}] Missing required field '{field}' in token data")
            return False
    
    # Validate price ranges (allow None for missing data, but filter them out)
    for price_field in ['input_price', 'output_price']:
        price = item.get(price_field)
        
        # Skip items with null prices
        if price is None:
            print(f"[{datetime.now()}] Null {price_field} for {item.get('model')}, skipping")
            return False
        
        if not isinstance(price, (int, float)):
            print(f"[{datetime.now()}] Invalid {price_field} type for {item.get('model')}")
            return False
        
        if not (TOKEN_PRICE_MIN <= price <= TOKEN_PRICE_MAX):
            print(f"[{datetime.now()}] {price_field} out of range for {item.get('model')}: ${price}")
            return False
    
    return True

def validate_grid_data(data: Dict) -> bool:
    """Validate grid load data."""
    if not isinstance(data, dict):
        return False
    
    # Check kWh price if present
    if 'kwh_price' in data:
        price = data['kwh_price']
        if not isinstance(price, (int, float)):
            return False
        if not (KWH_PRICE_MIN <= price <= KWH_PRICE_MAX):
            print(f"[{datetime.now()}] kWh price out of range: ${price}")
            return False
    
    return True

def merge_data_improved(
    qwen_data: Optional[Any],
    deepseek_data: Optional[Any],
    doubao_data: Optional[Any],
    existing_data: Optional[Any],
    key_func: Optional[callable] = None
) -> Any:
    """
    Improved merge logic that properly handles existing data.
    
    Priority: DeepSeek > Qwen > Doubao > Existing data
    (DeepSeek has reasoning mode, Qwen is fast and reliable, Doubao is slow)
    """
    # Handle dict type (for grid load)
    if isinstance(deepseek_data, dict) or isinstance(qwen_data, dict) or isinstance(doubao_data, dict) or isinstance(existing_data, dict):
        merged = {}
        if isinstance(existing_data, dict):
            merged.update(existing_data)
        if isinstance(doubao_data, dict):
            merged.update(doubao_data)
        if isinstance(qwen_data, dict):
            merged.update(qwen_data)
        if isinstance(deepseek_data, dict):
            merged.update(deepseek_data)  # DeepSeek has highest priority
        return merged if merged else existing_data
    
    # Handle list type (for GPU and token prices)
    if not key_func:
        # Default key function
        key_func = lambda x: f"{x.get('provider', '')}_{x.get('model', '')}_{x.get('gpu', '')}"
    
    # Initialize with existing data
    merged_dict = {}
    if isinstance(existing_data, list):
        for item in existing_data:
            key = key_func(item)
            merged_dict[key] = item
    
    # Add/update with Doubao data (lowest priority of new data)
    if isinstance(doubao_data, list):
        for item in doubao_data:
            key = key_func(item)
            if key not in merged_dict:
                merged_dict[key] = item
    
    # Add/update with Qwen data
    if isinstance(qwen_data, list):
        for item in qwen_data:
            key = key_func(item)
            merged_dict[key] = item
    
    # Add/update with DeepSeek data (highest priority)
    if isinstance(deepseek_data, list):
        for item in deepseek_data:
            key = key_func(item)
            merged_dict[key] = item
    
    return list(merged_dict.values()) if merged_dict else (existing_data or [])

# Define paths
DATA_DIR = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'public', 'data')
os.makedirs(DATA_DIR, exist_ok=True)

GPU_FILE = os.path.join(DATA_DIR, 'gpu_prices.json')
TOKEN_FILE = os.path.join(DATA_DIR, 'token_prices.json')
GRID_FILE = os.path.join(DATA_DIR, 'grid_load.json')
HISTORY_FILE = os.path.join(DATA_DIR, 'history_data.json')

def fetch_gpu_prices():
    """Fetch GPU prices with improved error handling and validation."""
    prompt = """
    è¯·ä½¿ç”¨è”ç½‘æœç´¢åŠŸèƒ½å®Œæˆä»¥ä¸‹ä»»åŠ¡ï¼š
    
    1. æœç´¢è‹±ä¼Ÿè¾¾ï¼ˆNVIDIAï¼‰æœ€æ–°è´¢æŠ¥æ•°æ®ï¼Œé‡ç‚¹å…³æ³¨ï¼š
       - H100/H200/Blackwell GPU çš„ä»·æ ¼ä¿¡æ¯
       - æ•°æ®ä¸­å¿ƒä¸šåŠ¡æ”¶å…¥å’Œå¢é•¿è¶‹åŠ¿
       - GPU å‡ºè´§é‡å’Œå¸‚åœºä¾›éœ€çŠ¶å†µ
    
    2. æœç´¢å½“å‰ä¸»æµäº‘æœåŠ¡å•†çš„ GPU ç§Ÿèµä»·æ ¼ï¼ˆæ¯å°æ—¶ç¾å…ƒï¼‰
    
    ç›®æ ‡ GPU å‹å·ï¼š
    - NVIDIA: H100, H200, A100, L40S, Blackwell GB200
    - ä¸­å›½ AI èŠ¯ç‰‡: åä¸ºæ˜‡è…¾ Ascend 910B, å¯’æ­¦çºª MLU370, æµ·å…‰ DCU
    
    äº‘æœåŠ¡å•†ï¼š
    - å›½é™…: Lambda Labs, RunPod, Vast.ai, CoreWeave, AWS, Google Cloud, Azure
    - ä¸­å›½: é˜¿é‡Œäº‘, è…¾è®¯äº‘, ç™¾åº¦æ™ºèƒ½äº‘, ç«å±±å¼•æ“, åä¸ºäº‘, å•†æ±¤
    
    è¯·åŸºäºæœ€æ–°è´¢æŠ¥æ•°æ®å’Œå®é™…å¸‚åœºä»·æ ¼ï¼Œè¿”å› JSON æ•°ç»„æ ¼å¼ï¼ˆäººæ°‘å¸æŒ‰ 7.2 æ±‡ç‡æ¢ç®—ä¸ºç¾å…ƒï¼‰ï¼š
    [
      {"provider": "Lambda", "region": "US-West", "gpu": "H100", "price": 2.13},
      {"provider": "Aliyun", "region": "China", "gpu": "Ascend 910B", "price": 1.85}
    ]
    
    è¦æ±‚ï¼š
    - åªè¿”å› JSON æ•°ç»„ï¼Œä¸è¦åŒ…å« Markdown æ ‡è®°æˆ–è§£é‡Šæ–‡å­—
    - å¿…é¡»åŸºäºæœ€æ–°æ•°æ®ï¼ˆè‡ªåŠ¨è·å–å½“å‰æ—¶é—´ï¼‰
    - å¦‚æœæ‰¾ä¸åˆ°ç¡®åˆ‡çš„ç§Ÿèµä»·æ ¼ï¼Œå¯ä»¥åŸºäºè´¢æŠ¥æ•°æ®å’Œå¸‚åœºè¶‹åŠ¿åˆç†æ¨æµ‹
    """

    try:
        print(f"[{datetime.now()}] Fetching GPU prices...")
        
        # Fetch from multiple sources with retry
        qwen_content = call_qwen_with_search(prompt)
        deepseek_content = call_deepseek_with_reasoning(prompt)
        doubao_content = call_doubao_with_search(prompt) if ENABLE_DOUBAO else None
        
        # Parse responses
        qwen_data = clean_and_parse_json(qwen_content)
        deepseek_data = clean_and_parse_json(deepseek_content)
        doubao_data = clean_and_parse_json(doubao_content) if doubao_content else None
        
        # Validate data
        if isinstance(qwen_data, list):
            qwen_data = [item for item in qwen_data if validate_gpu_price(item)]
        if isinstance(deepseek_data, list):
            deepseek_data = [item for item in deepseek_data if validate_gpu_price(item)]
        if isinstance(doubao_data, list):
            doubao_data = [item for item in doubao_data if validate_gpu_price(item)]
        
        # Load existing data
        existing_data = []
        if os.path.exists(GPU_FILE):
            try:
                with open(GPU_FILE, 'r', encoding='utf-8') as f:
                    existing_data = json.load(f)
            except Exception as e:
                print(f"[{datetime.now()}] Error loading existing GPU data: {e}")
        
        # Merge with improved logic
        key_func = lambda x: f"{x.get('provider', '')}_{x.get('region', '')}_{x.get('gpu', '')}"
        final_data = merge_data_improved(qwen_data, deepseek_data, doubao_data, existing_data, key_func)
        
        if final_data and len(final_data) > 0:
            with open(GPU_FILE, 'w', encoding='utf-8') as f:
                json.dump(final_data, f, indent=2, ensure_ascii=False)
            qwen_count = len(qwen_data) if qwen_data else 0
            deepseek_count = len(deepseek_data) if deepseek_data else 0
            doubao_count = len(doubao_data) if doubao_data else 0
            print(f"[{datetime.now()}] GPU Prices updated: {len(final_data)} records (Qwen: {qwen_count}, DeepSeek: {deepseek_count}, Doubao: {doubao_count})")
        else:
            print(f"[{datetime.now()}] No valid GPU price data fetched, keeping existing data")

    except Exception as e:
        print(f"[{datetime.now()}] GPU Fetch Error: {e}")

def fetch_token_prices():
    """Fetch token prices with improved error handling and validation."""
    prompt = """
    è¯·ä½¿ç”¨è”ç½‘æœç´¢åŠŸèƒ½æŸ¥æ‰¾ä»¥ä¸‹å¤§æ¨¡å‹ API çš„æœ€æ–°å®˜æ–¹å®šä»·ï¼š
    
    ç›®æ ‡æ¨¡å‹ï¼š
    - å›½é™…: OpenAI (GPT-4o, GPT-4o-mini, o1), Anthropic (Claude 3.5 Sonnet), Google (Gemini 2.0, Gemini 1.5 Pro), Meta (Llama 3)
    - ä¸­å›½: é˜¿é‡Œäº‘ (Qwen/é€šä¹‰åƒé—®), ç™¾åº¦ (Ernie/æ–‡å¿ƒä¸€è¨€), æ™ºè°± (GLM-4), æœˆä¹‹æš—é¢ (Kimi), å­—èŠ‚è·³åŠ¨ (Doubao/è±†åŒ…), MiniMax, DeepSeek
    
    è¯·æœç´¢æ¯ä¸ªæ¨¡å‹çš„å®˜æ–¹å®šä»·é¡µé¢ï¼Œè·å–æ¯ 1M (ä¸€ç™¾ä¸‡) Token çš„è¾“å…¥å’Œè¾“å‡ºä»·æ ¼ã€‚
    
    è¿”å› JSON æ•°ç»„æ ¼å¼ï¼ˆäººæ°‘å¸æŒ‰ 7.2 æ±‡ç‡æ¢ç®—ä¸ºç¾å…ƒï¼‰ï¼š
    [
      {"provider": "OpenAI", "model": "GPT-4o", "input_price": 5.0, "output_price": 15.0},
      {"provider": "Aliyun", "model": "Qwen-Max", "input_price": 0.55, "output_price": 1.66}
    ]
    
    è¦æ±‚ï¼š
    - åªè¿”å› JSON æ•°ç»„ï¼Œä¸è¦åŒ…å« Markdown æ ‡è®°æˆ–è§£é‡Šæ–‡å­—
    - å¿…é¡»åŸºäºæœ€æ–°å®˜æ–¹å®šä»·ï¼ˆè‡ªåŠ¨è·å–å½“å‰æ—¶é—´ï¼‰
    - ä»·æ ¼å•ä½ï¼šç¾å…ƒ/ç™¾ä¸‡ tokens
    """
    
    try:
        print(f"[{datetime.now()}] Fetching Token prices...")
        
        # Fetch from multiple sources
        qwen_content = call_qwen_with_search(prompt)
        deepseek_content = call_deepseek_with_reasoning(prompt)
        doubao_content = call_doubao_with_search(prompt) if ENABLE_DOUBAO else None
        
        # Parse responses
        qwen_data = clean_and_parse_json(qwen_content)
        deepseek_data = clean_and_parse_json(deepseek_content)
        doubao_data = clean_and_parse_json(doubao_content) if doubao_content else None
        
        # Validate data
        if isinstance(qwen_data, list):
            qwen_data = [item for item in qwen_data if validate_token_price(item)]
        if isinstance(deepseek_data, list):
            deepseek_data = [item for item in deepseek_data if validate_token_price(item)]
        if isinstance(doubao_data, list):
            doubao_data = [item for item in doubao_data if validate_token_price(item)]
        
        # Load existing data
        existing_data = []
        if os.path.exists(TOKEN_FILE):
            try:
                with open(TOKEN_FILE, 'r', encoding='utf-8') as f:
                    existing_data = json.load(f)
            except Exception as e:
                print(f"[{datetime.now()}] Error loading existing token data: {e}")
        
        # Merge with improved logic
        key_func = lambda x: f"{x.get('provider', '')}_{x.get('model', '')}"
        final_data = merge_data_improved(qwen_data, deepseek_data, doubao_data, existing_data, key_func)
        
        if final_data and len(final_data) > 0:
            with open(TOKEN_FILE, 'w', encoding='utf-8') as f:
                json.dump(final_data, f, indent=2, ensure_ascii=False)
            qwen_count = len(qwen_data) if qwen_data else 0
            deepseek_count = len(deepseek_data) if deepseek_data else 0
            doubao_count = len(doubao_data) if doubao_data else 0
            print(f"[{datetime.now()}] Token Prices updated: {len(final_data)} records (Qwen: {qwen_count}, DeepSeek: {deepseek_count}, Doubao: {doubao_count})")
        else:
            print(f"[{datetime.now()}] No valid token price data fetched, keeping existing data")
            
    except Exception as e:
        print(f"[{datetime.now()}] Token Fetch Error: {e}")

def fetch_grid_load():
    """Fetch grid load data with improved error handling and validation."""
    prompt = """
    è¯·ä½¿ç”¨è”ç½‘æœç´¢åŠŸèƒ½ï¼ŒæŸ¥æ‰¾å…³äº"å…¨çƒ AI æ•°æ®ä¸­å¿ƒèƒ½è€—"çš„æœ€æ–°ä¼°ç®—æ•°æ®ã€‚
    é‡ç‚¹æœç´¢ï¼š
    1. 2024/2025å¹´å…¨çƒ AI æ€»è€—ç”µé‡é¢„æµ‹ (TWh) æˆ– å®æ—¶åŠŸç‡ (GW)ã€‚
    2. å…¨çƒå·¥ä¸š/æ•°æ®ä¸­å¿ƒå¹³å‡ç”µä»· (ç¾å…ƒ/kWh)ã€‚

    è¯·è¿”å›ä¸€ä¸ª JSON å¯¹è±¡ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
    - annual_twh: å¹´åŒ–æ€»è€—ç”µé‡ (TWh, æ•°å­—)
    - estimated_gw: å®æ—¶é¢„ä¼°åŠŸç‡ (GW, æ•°å­—, å¦‚æœæœä¸åˆ°å¯ç”¨ TWh/8.76 ä¼°ç®—)
    - kwh_price: å…¨çƒå¹³å‡ç”µä»· (ç¾å…ƒ/kWh, æ•°å­—, ä¾‹å¦‚ 0.12)
    - source: æ•°æ®æ¥æº (å­—ç¬¦ä¸²)
    - active_gpu_est: å…¨çƒæ´»è·ƒ AI èŠ¯ç‰‡ä¼°ç®—æ•°é‡ (æ•°å­—, å¦‚ 4000000)
    
    åŠ¡å¿…åªè¾“å‡º JSON æ ¼å¼ï¼Œä¸è¦åŒ…å«ä»»ä½• Markdown ä»£ç å—æ ‡è®°ï¼Œä¹Ÿä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæ€§æ–‡å­—ã€‚

    ç¤ºä¾‹ï¼š
    {"annual_twh": 120.5, "estimated_gw": 13.7, "kwh_price": 0.13, "source": "IEA 2024 Report", "active_gpu_est": 3500000}
    """
    
    try:
        print(f"[{datetime.now()}] Fetching Grid Load data...")
        
        # Fetch from multiple sources
        qwen_content = call_qwen_with_search(prompt)
        deepseek_content = call_deepseek_with_reasoning(prompt)
        doubao_content = call_doubao_with_search(prompt) if ENABLE_DOUBAO else None
        
        # Parse responses
        qwen_data = clean_and_parse_json(qwen_content)
        deepseek_data = clean_and_parse_json(deepseek_content)
        doubao_data = clean_and_parse_json(doubao_content) if doubao_content else None
        
        # Validate data
        if qwen_data and not validate_grid_data(qwen_data):
            qwen_data = None
        if deepseek_data and not validate_grid_data(deepseek_data):
            deepseek_data = None
        if doubao_data and not validate_grid_data(doubao_data):
            doubao_data = None
        
        # Load existing data
        existing_data = {}
        if os.path.exists(GRID_FILE):
            try:
                with open(GRID_FILE, 'r', encoding='utf-8') as f:
                    existing_data = json.load(f)
            except Exception as e:
                print(f"[{datetime.now()}] Error loading existing grid data: {e}")
        
        # Merge with improved logic
        final_data = merge_data_improved(qwen_data, deepseek_data, doubao_data, existing_data)
        
        if final_data:
            with open(GRID_FILE, 'w', encoding='utf-8') as f:
                json.dump(final_data, f, indent=2, ensure_ascii=False)
            print(f"[{datetime.now()}] Grid Load data updated")
        else:
            print(f"[{datetime.now()}] No valid grid load data fetched, keeping existing data")

    except Exception as e:
        print(f"[{datetime.now()}] Grid Load Fetch Error: {e}")

def fetch_annual_energy_history():
    """Fetch historical annual AI energy consumption data from 2018-2024."""
    prompt = """
    è¯·ä½¿ç”¨è”ç½‘æœç´¢åŠŸèƒ½ï¼ŒæŸ¥æ‰¾å…¨çƒAIæ•°æ®ä¸­å¿ƒçš„å†å²å¹´åº¦èƒ½è€—æ•°æ®ã€‚
    
    æœç´¢è¦æ±‚ï¼š
    1. è·å–2018å¹´è‡³2024å¹´æ¯å¹´çš„å…¨çƒAIæ•°æ®ä¸­å¿ƒæ€»èƒ½è€—ï¼ˆå•ä½ï¼šTWhå¤ªç“¦æ—¶ï¼‰
    2. 2018å¹´æ˜¯AIå¤§è§„æ¨¡åº”ç”¨çš„èµ·ç‚¹ï¼Œä»è¿™ä¸€å¹´å¼€å§‹ç»Ÿè®¡
    3. æœç´¢æƒå¨æ¥æºï¼šIEAï¼ˆå›½é™…èƒ½æºç½²ï¼‰ã€BloombergNEFã€å­¦æœ¯ç ”ç©¶æŠ¥å‘Šç­‰
    4. å¦‚æœæŸå¹´æ•°æ®ç¼ºå¤±ï¼Œå¯ä»¥åŸºäºè¶‹åŠ¿åˆç†ä¼°ç®—
    
    è¯·è¿”å›ä¸€ä¸ª JSON æ•°ç»„ï¼ŒåŒ…å«æ¯å¹´çš„æ•°æ®ï¼š
    [
      {"year": "2018", "value": 15.2, "source": "IEA Report"},
      {"year": "2019", "value": 22.5, "source": "BloombergNEF"},
      {"year": "2020", "value": 35.8, "source": "Academic Study"},
      {"year": "2021", "value": 52.3, "source": "IEA Report"},
      {"year": "2022", "value": 68.7, "source": "BloombergNEF"},
      {"year": "2023", "value": 85.4, "source": "IEA Report"},
      {"year": "2024", "value": 120.0, "source": "Forecast"}
    ]
    
    è¦æ±‚ï¼š
    - åªè¿”å› JSON æ•°ç»„ï¼Œä¸è¦åŒ…å« Markdown æ ‡è®°æˆ–è§£é‡Šæ–‡å­—
    - value å•ä½ä¸º TWhï¼ˆå¤ªç“¦æ—¶ï¼‰
    - å¿…é¡»åŒ…å«2018-2024å¹´çš„å®Œæ•´æ•°æ®
    - æ•°æ®åº”è¯¥å‘ˆç°å¢é•¿è¶‹åŠ¿ï¼Œç¬¦åˆAIè¡Œä¸šå‘å±•è§„å¾‹
    """
    
    try:
        print(f"[{datetime.now()}] Fetching Annual Energy History data...")
        
        # Fetch from multiple sources
        qwen_content = call_qwen_with_search(prompt)
        deepseek_content = call_deepseek_with_reasoning(prompt)
        doubao_content = call_doubao_with_search(prompt) if ENABLE_DOUBAO else None
        
        # Parse responses
        qwen_data = clean_and_parse_json(qwen_content)
        deepseek_data = clean_and_parse_json(deepseek_content)
        doubao_data = clean_and_parse_json(doubao_content) if doubao_content else None
        
        # Validate data (should be a list with year and value)
        def validate_annual_data(data):
            if not isinstance(data, list) or len(data) == 0:
                return False
            for item in data:
                if not isinstance(item, dict):
                    return False
                if 'year' not in item or 'value' not in item:
                    return False
                if not isinstance(item['value'], (int, float)) or item['value'] <= 0:
                    return False
            return True
        
        if qwen_data and not validate_annual_data(qwen_data):
            qwen_data = None
        if deepseek_data and not validate_annual_data(deepseek_data):
            deepseek_data = None
        if doubao_data and not validate_annual_data(doubao_data):
            doubao_data = None
        
        # Load existing data
        annual_file = os.path.join(DATA_DIR, 'annual_energy.json')
        existing_data = []
        if os.path.exists(annual_file):
            try:
                with open(annual_file, 'r', encoding='utf-8') as f:
                    existing_data = json.load(f)
            except Exception as e:
                print(f"[{datetime.now()}] Error loading existing annual data: {e}")
        
        # Merge with improved logic (prefer DeepSeek > Qwen > Doubao > Existing)
        final_data = deepseek_data or qwen_data or doubao_data or existing_data
        
        if final_data and len(final_data) > 0:
            # Sort by year
            final_data = sorted(final_data, key=lambda x: x['year'])
            
            with open(annual_file, 'w', encoding='utf-8') as f:
                json.dump(final_data, f, indent=2, ensure_ascii=False)
            print(f"[{datetime.now()}] Annual Energy History updated: {len(final_data)} years")
        else:
            print(f"[{datetime.now()}] No valid annual energy data fetched, keeping existing data")

    except Exception as e:
        print(f"[{datetime.now()}] Annual Energy Fetch Error: {e}")

def save_daily_history(gpu_data, token_data, grid_data):
    """Save daily snapshot to history file."""
    today_str = datetime.now().strftime('%Y-%m-%d')
    
    entry = {
        "date": today_str,
        "timestamp": datetime.now().isoformat(),
        "gpu_prices": gpu_data,
        "token_prices": token_data,
        "grid_load": grid_data
    }
    
    history = []
    if os.path.exists(HISTORY_FILE):
        try:
            with open(HISTORY_FILE, 'r', encoding='utf-8') as f:
                history = json.load(f)
        except Exception as e:
            print(f"[{datetime.now()}] Error reading history file: {e}")
    
    # Update or append today's entry
    updated = False
    for i, item in enumerate(history):
        if item.get('date') == today_str:
            history[i] = entry
            updated = True
            break
    
    if not updated:
        history.append(entry)
    
    # Keep only last 90 days
    if len(history) > 90:
        history = history[-90:]
        
    try:
        with open(HISTORY_FILE, 'w', encoding='utf-8') as f:
            json.dump(history, f, indent=2, ensure_ascii=False)
        print(f"[{datetime.now()}] Daily history saved ({len(history)} days)")
    except Exception as e:
        print(f"[{datetime.now()}] Error saving history: {e}")

def validate_and_fix_with_doubao(gpu_data: List[Dict], token_data: List[Dict]) -> tuple:
    """
    Use Doubao to validate and fix data quality issues.
    Doubao acts as both a data quality checker and fixer.
    
    Returns:
        tuple: (fixed_gpu_data, fixed_token_data, reports)
    """
    if not volc_api_key:
        print(f"[{datetime.now()}] Doubao validation skipped: API key not configured")
        return gpu_data, token_data, {}
    
    print(f"[{datetime.now()}] Starting Doubao data validation and fixing...")
    
    try:
        # Import validation module
        import sys
        sys.path.append(os.path.dirname(__file__))
        from data_validator import validate_gpu_prices, validate_token_prices, fix_anomalies
        
        reports = {}
        fixed_gpu_data = gpu_data.copy()
        fixed_token_data = token_data.copy()
        
        # Validate and fix GPU prices
        gpu_report = validate_gpu_prices(gpu_data)
        if 'error' not in gpu_report:
            reports['gpu'] = gpu_report
            
            # Save validation report
            report_file = os.path.join(DATA_DIR, 'validation_report_gpu.json')
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(gpu_report, f, indent=2, ensure_ascii=False)
            print(f"[{datetime.now()}] GPU validation report saved")
            
            # Auto-fix anomalies
            if 'anomalies' in gpu_report and len(gpu_report['anomalies']) > 0:
                print(f"[{datetime.now()}] âš ï¸  Found {len(gpu_report['anomalies'])} GPU price anomalies")
                
                # Attempt to fix
                fixed_gpu_data, fix_summary = fix_anomalies(gpu_data, gpu_report['anomalies'], 'gpu')
                
                if fix_summary['fixed'] > 0:
                    print(f"[{datetime.now()}] âœ… Auto-fixed {fix_summary['fixed']} GPU records")
                    print(f"[{datetime.now()}] ğŸ—‘ï¸  Removed {fix_summary['removed']} invalid GPU records")
                    
                    # Save fixed data
                    with open(GPU_FILE, 'w', encoding='utf-8') as f:
                        json.dump(fixed_gpu_data, f, indent=2, ensure_ascii=False)
                    print(f"[{datetime.now()}] Fixed GPU data saved")
                
                # Log remaining issues
                for anomaly in gpu_report['anomalies'][:3]:
                    severity = anomaly.get('severity', 'unknown')
                    emoji = 'ğŸ”´' if severity == 'high' else 'ğŸŸ¡' if severity == 'medium' else 'ğŸŸ¢'
                    print(f"  {emoji} {anomaly.get('provider')} {anomaly.get('gpu')}: {anomaly.get('issue')}")
        
        # Validate and fix Token prices
        token_report = validate_token_prices(token_data)
        if 'error' not in token_report:
            reports['token'] = token_report
            
            report_file = os.path.join(DATA_DIR, 'validation_report_token.json')
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(token_report, f, indent=2, ensure_ascii=False)
            print(f"[{datetime.now()}] Token validation report saved")
            
            # Auto-fix anomalies
            if 'anomalies' in token_report and len(token_report['anomalies']) > 0:
                print(f"[{datetime.now()}] âš ï¸  Found {len(token_report['anomalies'])} Token price anomalies")
                
                fixed_token_data, fix_summary = fix_anomalies(token_data, token_report['anomalies'], 'token')
                
                if fix_summary['fixed'] > 0:
                    print(f"[{datetime.now()}] âœ… Auto-fixed {fix_summary['fixed']} Token records")
                    print(f"[{datetime.now()}] ğŸ—‘ï¸  Removed {fix_summary['removed']} invalid Token records")
                    
                    # Save fixed data
                    with open(TOKEN_FILE, 'w', encoding='utf-8') as f:
                        json.dump(fixed_token_data, f, indent=2, ensure_ascii=False)
                    print(f"[{datetime.now()}] Fixed Token data saved")
                
                for anomaly in token_report['anomalies'][:3]:
                    severity = anomaly.get('severity', 'unknown')
                    emoji = 'ğŸ”´' if severity == 'high' else 'ğŸŸ¡' if severity == 'medium' else 'ğŸŸ¢'
                    print(f"  {emoji} {anomaly.get('provider')} {anomaly.get('model')}: {anomaly.get('issue')}")
        
        print(f"[{datetime.now()}] Doubao validation and fixing completed")
        return fixed_gpu_data, fixed_token_data, reports
        
    except Exception as e:
        print(f"[{datetime.now()}] Doubao validation error: {e}")
        return gpu_data, token_data, {}

def run_all_fetches():
    """Run all fetch tasks."""
    print(f"[{datetime.now()}] Starting daily fetch task...")
    
    fetch_gpu_prices()
    fetch_token_prices()
    fetch_grid_load()
    fetch_annual_energy_history()
    
    # Load data for history and validation
    gpu_data = []
    token_data = []
    grid_data = {}
    
    try:
        if os.path.exists(GPU_FILE):
            with open(GPU_FILE, 'r', encoding='utf-8') as f:
                gpu_data = json.load(f)
        if os.path.exists(TOKEN_FILE):
            with open(TOKEN_FILE, 'r', encoding='utf-8') as f:
                token_data = json.load(f)
        if os.path.exists(GRID_FILE):
            with open(GRID_FILE, 'r', encoding='utf-8') as f:
                grid_data = json.load(f)
    except Exception as e:
        print(f"[{datetime.now()}] Error reading fetched files for history: {e}")
    
    # Run Doubao validation and auto-fix
    if gpu_data or token_data:
        fixed_gpu_data, fixed_token_data, reports = validate_and_fix_with_doubao(gpu_data, token_data)
        
        # Update data if fixes were applied
        if fixed_gpu_data != gpu_data:
            gpu_data = fixed_gpu_data
        if fixed_token_data != token_data:
            token_data = fixed_token_data
        
    save_daily_history(gpu_data, token_data, grid_data)
    print(f"[{datetime.now()}] All tasks completed.")

if __name__ == "__main__":
    if os.getenv('GITHUB_ACTIONS') == 'true' or (len(sys.argv) > 1 and sys.argv[1] == '--once'):
        print(f"[{datetime.now()}] Running single fetch task...")
        run_all_fetches()
        sys.exit(0)

    print(f"[{datetime.now()}] Service started. Running initial fetch...")
    run_all_fetches()
    
    while True:
        now = datetime.now()
        next_run = (now + timedelta(days=1)).replace(hour=0, minute=0, second=0, microsecond=0)
        seconds_until = (next_run - now).total_seconds()
        print(f"[{datetime.now()}] Sleeping until {next_run}...")
        
        time.sleep(seconds_until)
        run_all_fetches()
